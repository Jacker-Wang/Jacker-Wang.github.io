# 添加yarn队列

配置YARN队列调度策略：

（1）配置项(此项可以在ambari上面yarn的customer yarn-site中配置)：
```
yarn.resourcemanager.scheduler.class
```
值：
```
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler
```
（2）配置项(此项可以在ambari上面yarn的customer yarn-site中配置)：
```
yarn.scheduler.fair.allocation.file
```
值：
```
/etc/hadoop/conf/fair-scheduler.xml
```
3.创建YARN队列配置文件：

在YARN所有ResourceManager主机/etc/hadoop/conf/下创建文件fair-scheduler.xml与mapred-queues.xml，内容示例如下。

default是默认队列，下面的配置添加了alogic队列

（1）fair-scheduler.xml
内容示例：
```
<?xml version="1.0"?>
<allocations>
 <queue name="default">
    <minResources>600000mb,0vcores</minResources>
    <schedulingPolicy>fair</schedulingPolicy>
    <aclAdministerApps>hadoop</aclAdministerApps>
    <aclSubmitApps>*</aclSubmitApps>
  </queue>

 <queue name="hive">
    <minResources>204800mb,10vcores</minResources>
    <maxResources>307200mb,20vcores</maxResources>
    <schedulingPolicy>fair</schedulingPolicy>
    <aclAdministerApps>hadoop</aclAdministerApps>
    <aclSubmitApps>hadoop,hive</aclSubmitApps>
  </queue>

  <queuePlacementPolicy>
    <rule name="specified" />
    <rule name="primaryGroup" create="false" />
    <rule name="default" />
  </queuePlacementPolicy>
</allocations>
```
队列内部配置文件 fair-scheduler.xml 说明

详细配置说明参考 [这里](https://www.jianshu.com/p/34efa45621bd)
```
<allocations>
 <queue name="default">
    <minResources>600000mb,0vcores</minResources>
    <schedulingPolicy>fair</schedulingPolicy>
    <aclAdministerApps>hadoop</aclAdministerApps>
    <aclSubmitApps>*</aclSubmitApps>
  </queue>

 <queue name="hive">
    <!--最小资源-->
    <minResources>204800mb,10vcores</minResources>
    <!--最大资源-->
    <maxResources>307200mb,20vcores</maxResources>
    <!--队列内部调度策略 （fair公平策略）-->
    <schedulingPolicy>fair</schedulingPolicy>
    <!--管理用户（可以kill application的用户）-->
    <aclAdministerApps>hadoop</aclAdministerApps>
    <!--可以向此队列提交的用户，但是这里如果是通过jdbc连接的应用（一般是hive用户）这里是不会提交到hive队列的-->
    <aclSubmitApps>hadoop,hive</aclSubmitApps>
  </queue>

  <queuePlacementPolicy>
    <!--首先匹配应用里面指定的队列-->
    <rule name="specified" />
    <!--其次匹配以用户名为名称的队列-->
    <rule name="primaryGroup" create="false" />
    <!--最后匹配 default 队列-->
    <rule name="default" />
  </queuePlacementPolicy>
</allocations>
```

（2）mapred-queues.xml（这个文件可以不需要存在）
```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<queues>
  <queue>
    <name>default</name>
    <state>running</state>
    <acl-submit-job> </acl-submit-job>
    <acl-administer-jobs> </acl-administer-jobs>
  </queue>
  <queue>
    <name>alogic</name>
    <state>running</state>
    <acl-submit-job>alogic</acl-submit-job>
    <acl-administer-jobs>alogic,hue</acl-administer-jobs>
   </queue>
</queues>
```

若需要添加YARN队列，则在/etc/hadoop/conf/fair-scheduler.xm中仿照示例添加<queue>标签内容。并且在/etc/hadoop/conf/mapred-queues.xml中添加相关队列。

# HA状态下的namenode格式化

1 停止所有namenode（stanby namenode和active namenode）

2 删除所有主机的datanode数据目录和namenode目录

3 开启所有的Journalnode服务

4 格式化zkfc

注意这里格式化所有的azfc（一般是2台）
```
  hdfs zkfc -formatZK
```

5 格式化namenode

 ```
  hdfs namenode -format
 ```

 6 拷贝namenode文件导另外一台namenode

    拷贝目录/~/hadoop/hdfs/namenode下面的所有文件。

 7 启动namenode
# JournalNode的API操作
## 增加Journalnode

 1  为增加Journalnode分配角色

```
curl -u admin:admin -H "X-Requested-By: Ambari" -X POST http://10.251.44.62:8181/api/v1/clusters/wss_cluster/hosts/s4.wss.com/host_components/JOURNALNODE
```
2  安装Journalnode
```
curl -u admin:admin -H 'X-Requested-By: Ambari' -X PUT -d '{"RequestInfo":{"context":"Install JournalNode"},"Body":{"HostRoles":{"state":"INSTALLED"}}}' http://10.251.44.62:8181/api/v1/clusters/wss_cluster/hosts/s4.wss.com/host_components/JOURNALNODE
```

3 更新hdfs的配置

    搜索配置项 dfs.namenode.shared.edits.dir ，添加新增的Journalnode主机和端口

4  创建Journalnode目录

    搜索配置项 dfs.journalnode.edits.dir ，查看对应的目录，然后创建目录。 搜索配置项 dfs.nameservices的值，在上一步创建的目录下面创建一个目录，目录名为这个值。

5 修改刚刚创建的目录的权限

  具体权限可以根据服务正常的Journalnode相应目录的权限修改

6 同步数据

  将服务正常的Journalnode目录下面的current目录中的所有文件拷贝过来，然后修改权限。

7 启动Ambari

   在Ambari界面启动新添加的Journalnode，然后查看日志。

##  删除journalnode服务

ambari2.4.2的UI 界面上应该是不能够直接删除journalnode服务的，所以这里通过API删除。
```
curl -u admin:admin -H "X-Requested-By: ambari" -X DELETE http://10.251.44.62:8181/api/v1/clusters/wss_cluster/hosts/s2.wss.com/host_components/JOURNALNODE
```
## 查看journalnode服务存在于哪些节点上
```
curl -u admin:admin -i -X GET http://10.251.44.62:8181/api/v1/clusters/wss_cluster/host_components?HostRoles/component_name=JOURNALNODE
```
## 查看一个需删除的journalnode节点详细信息

```
curl -u admin:admin -H "X-Requested-By: ambari" -X GET http://10.251.44.62:8181/api/v1/clusters/wss_cluster/hosts/s2.wss.com/host_components/JOURNALNODE
```

# 解除和委任节点

  详细参考 [这里](https://blog.csdn.net/yangjjuan/article/details/71248626)

## 解除 NodeManager 节点
1 Ambari搜索
```
yarn.resourcemanager.nodes.exclude-path
```
 值一般是
```
  /etc/hadoop/conf/yarn.exclude。
```

2 在/etc/hadoop/conf/yarn.exclude文件中添加需要解除的NodeManager，一行一个节点。

3 ssh连接 resourcemanager，切换到yarn用户，使用命令
```
yarn rmadmin -refreshNodes
```
更新当前节点，即可解除节点。
## 解除 dataNode 节点

1 Ambari搜索
```
dfs.hosts.exclude
```
 值一般是
```
  /etc/hadoop/conf/dfs.exclude
```

2 在/etc/hadoop/conf/dfs.exclude文件中添加需要解除的dataManager，一行一个节点。

3 ssh连接 namenode，切换到hdfs用户，使用命令
```
hadoop dfsadmin -refreshNodes
```
更新当前节点，即可开始解除DataNode过程，解除的DataNode会把快信息复制到其他DataNode上

4 从slaves文件移除节点。

# 处理 Ambari错误
1 The 'krb5-conf' configuration is not available/The 'kerberos-env' configuration is not available

    这个问题一般会出现在禁用kerberos的过程当中，还会出现在利用启用了Kerberos的Ambari添加服务的过程当中。总之都是kerberos造成的原因。

  解决：

     进入Ambari数据库，将表 clusterconfigmpping 的selected全部更新为 1


```
select * from clusterconfigmapping WHERE type_name in ('kerberos-env', 'krb5-conf');

update clusterconfigmapping set selected=1  WHERE type_name in ('kerberos-env', 'krb5-conf');

```

最后重启ambari-server。

# 操作PostSql数据库


```
#切换用户到 ambari （如果没有就创建）
sudo su - ambari

#进入ambari数据库(默认密码是 bigdata)
psql
#此时就在 ambari 库中了

#赋予权限
grant all privileges on database xxf to xxf

#创建用户
create user xxf with password '******';

# \d 查看当前数据库的所有表
# \l 查看所有数据库
# \c ambari 连接到 ambari数据库
# 其它增删改查，创建用户，修改权限操作和mysql一样

```
